{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the DECODE OpenCloud Docs DECODE OpenCloud allows running algorithms on attached compute power in the spirit of SaaS. This documentation is mainly meant for developers. For users, you can find short high-level guides on how to use DECODE OpenCloud and attach new machine workers below. Running an implemented algorithm The easiest way is to use our minimal website frontend . The frontend contains guides on how to run the different supported algorithms. On a high level, you will need to: Register an account using the frontend (and wait for the account to be approved) Upload input data Start the job Retrieve the results after the job has finished (you will be notified by email) Attaching compute power Follow the usage instructions of the JobFetcher . You will need: A Linux (virtual) machine with: Docker CUDA hardware to run GPU jobs nvidia-container-toolkit to run GPU jobs A registered worker account (you can use the website frontend to register)","title":"Home"},{"location":"#welcome-to-the-decode-opencloud-docs","text":"DECODE OpenCloud allows running algorithms on attached compute power in the spirit of SaaS. This documentation is mainly meant for developers. For users, you can find short high-level guides on how to use DECODE OpenCloud and attach new machine workers below.","title":"Welcome to the DECODE OpenCloud Docs"},{"location":"#running-an-implemented-algorithm","text":"The easiest way is to use our minimal website frontend . The frontend contains guides on how to run the different supported algorithms. On a high level, you will need to: Register an account using the frontend (and wait for the account to be approved) Upload input data Start the job Retrieve the results after the job has finished (you will be notified by email)","title":"Running an implemented algorithm"},{"location":"#attaching-compute-power","text":"Follow the usage instructions of the JobFetcher . You will need: A Linux (virtual) machine with: Docker CUDA hardware to run GPU jobs nvidia-container-toolkit to run GPU jobs A registered worker account (you can use the website frontend to register)","title":"Attaching compute power"},{"location":"architecture/design/","text":"Design choices Architecture Pulling architecture of jobs The workers pull the jobs from the queue, instead of the queue pushing jobs to them. This way, the worker-facing API does not need a direct connection to the workers. New workers can be more easily added. Workers can pull jobs only whenever they have free resources, and do not get constant updates. Database as a queue Allows filtering by more criteria, including prioritization, resource matching, and so on. We will not have so many jobs that performance could significantly suffer. Technical implementation Different job IDs in the user-facing and worker-facing APIs The worker-facing API queue being a database is only one implementation, and might change in the future. Having own IDs decouples user- and worker-facing APIs. In particular, we might have different user-facing APIs using the same worker-facing API. Jobs can be deleted from the worker-facing API's database when they finished, while coupled IDs would ideally require consistency (1:1 relation) between the two databases. Cognito pool: \"users\" and \"workers\" groups \"users\" and \"workers\" groups are a nice way to allow users to self-register in the user pool and verify their email address, while not allowing free access to anyone. Docker with no entrypoint This is required because we cannot dynamically define a mount point when starting a job on AWS Batch (it would require a new job definition). This means that we need to mount a path on EFS that is common to all jobs: to separate the filesystem of each job, we create one folder within it per job. On the other hand, we want the user (and user-facing API) to define paths relative to a fixed path, not to a path dependent on the job ID. What we do to solve this is to prepend a command that points /files to the job-specific directory to the Docker command. An entrypoint would run before the command, and not have access to the correct filesystem. Pre-signed URLs for data down- and upload This avoids the data traffic passing through the API. Instead, the client directly connects to S3. Keep-alive signals These are implemented to notice silent failures of workers. A daemon on the worker-facing API periodically checks and requeues jobs that have not received status updates for a while.","title":"Design choices"},{"location":"architecture/design/#design-choices","text":"","title":"Design choices"},{"location":"architecture/design/#architecture","text":"","title":"Architecture"},{"location":"architecture/design/#pulling-architecture-of-jobs","text":"The workers pull the jobs from the queue, instead of the queue pushing jobs to them. This way, the worker-facing API does not need a direct connection to the workers. New workers can be more easily added. Workers can pull jobs only whenever they have free resources, and do not get constant updates.","title":"Pulling architecture of jobs"},{"location":"architecture/design/#database-as-a-queue","text":"Allows filtering by more criteria, including prioritization, resource matching, and so on. We will not have so many jobs that performance could significantly suffer.","title":"Database as a queue"},{"location":"architecture/design/#technical-implementation","text":"","title":"Technical implementation"},{"location":"architecture/design/#different-job-ids-in-the-user-facing-and-worker-facing-apis","text":"The worker-facing API queue being a database is only one implementation, and might change in the future. Having own IDs decouples user- and worker-facing APIs. In particular, we might have different user-facing APIs using the same worker-facing API. Jobs can be deleted from the worker-facing API's database when they finished, while coupled IDs would ideally require consistency (1:1 relation) between the two databases.","title":"Different job IDs in the user-facing and worker-facing APIs"},{"location":"architecture/design/#cognito-pool-users-and-workers-groups","text":"\"users\" and \"workers\" groups are a nice way to allow users to self-register in the user pool and verify their email address, while not allowing free access to anyone.","title":"Cognito pool: \"users\" and \"workers\" groups"},{"location":"architecture/design/#docker-with-no-entrypoint","text":"This is required because we cannot dynamically define a mount point when starting a job on AWS Batch (it would require a new job definition). This means that we need to mount a path on EFS that is common to all jobs: to separate the filesystem of each job, we create one folder within it per job. On the other hand, we want the user (and user-facing API) to define paths relative to a fixed path, not to a path dependent on the job ID. What we do to solve this is to prepend a command that points /files to the job-specific directory to the Docker command. An entrypoint would run before the command, and not have access to the correct filesystem.","title":"Docker with no entrypoint"},{"location":"architecture/design/#pre-signed-urls-for-data-down-and-upload","text":"This avoids the data traffic passing through the API. Instead, the client directly connects to S3.","title":"Pre-signed URLs for data down- and upload"},{"location":"architecture/design/#keep-alive-signals","text":"These are implemented to notice silent failures of workers. A daemon on the worker-facing API periodically checks and requeues jobs that have not received status updates for a while.","title":"Keep-alive signals"},{"location":"architecture/repositories/","text":"Repositories Overview The DECODE OpenCloud project is composed of multiple repositories: UserFrontend : Vue.js app for the user-facing website UserAPI : API called by the front-end application WorkerAPI : API called by the workers JobFetcher : python interface and Docker recipe for workers to run to fetch and process jobs AWS_Infrastructure : AWS infrastructure, defined using the Python CDK DECODE Cloud runs software in Docker containers with a compatible structure. Currently, it supports: DECODE : single-molecule localization microscopy DECODE Plex : single-molecule localization microscopy Comet : drift correction For development, the following repositories are additionally used: IntegrationTests : integration tests Documentation : defines this documentation Interdependencies The repositories are intertwined in the manners detailed below: The user-facing API and worker-facing API are aware of each other's URL , to be able to exchange information about the jobs. This gives rise to a circular dependency that is resolved by setting one environment variable in the CDK code directly and using a triggered function in the CDK code . The user-facing API and worker-facing API communicate via internal endpoints that should only be accessible to them. This is safeguarded using an API key ( here and here ), that is stored as a secret in AWS SecretsManager . The workers and frontend need the URLs of the worker-facing API and the user-facing API respectively , that are passed to them as environment variables ( here for local workers, here for cloud workers, and here for the frontend). Additionally, the frontend requires appropriate CORS middleware both on the user-facing API side and on the AWS S3 bucket side . Worker-facing API, user-facing API, user frontend all authenticate in an AWS Cognito user pool, defined in the AWS infrastructure repository . Users are divided into \"users\" and \"workers\"; additionally, cloud workers are identified by their membership in the \"cloud\" group. A user for the cloud workers is created with the CDK stack using a trigger . We typically store the Docker images of the supported applications in an AWS ECR repository (outside of the CDK stack). The AWS infrastructure repository contains a script to push local Docker images to a public ECR repository . Additionally, we provide scripts to link the APIs deployed on AWS to custom domains and setup the email sender for the user-facing API to send notifications to users . The AWS infrastructure repository deploys the APIs and Cloud Worker using their docker images published on ECR .","title":"Repositories"},{"location":"architecture/repositories/#repositories","text":"","title":"Repositories"},{"location":"architecture/repositories/#overview","text":"The DECODE OpenCloud project is composed of multiple repositories: UserFrontend : Vue.js app for the user-facing website UserAPI : API called by the front-end application WorkerAPI : API called by the workers JobFetcher : python interface and Docker recipe for workers to run to fetch and process jobs AWS_Infrastructure : AWS infrastructure, defined using the Python CDK DECODE Cloud runs software in Docker containers with a compatible structure. Currently, it supports: DECODE : single-molecule localization microscopy DECODE Plex : single-molecule localization microscopy Comet : drift correction For development, the following repositories are additionally used: IntegrationTests : integration tests Documentation : defines this documentation","title":"Overview"},{"location":"architecture/repositories/#interdependencies","text":"The repositories are intertwined in the manners detailed below: The user-facing API and worker-facing API are aware of each other's URL , to be able to exchange information about the jobs. This gives rise to a circular dependency that is resolved by setting one environment variable in the CDK code directly and using a triggered function in the CDK code . The user-facing API and worker-facing API communicate via internal endpoints that should only be accessible to them. This is safeguarded using an API key ( here and here ), that is stored as a secret in AWS SecretsManager . The workers and frontend need the URLs of the worker-facing API and the user-facing API respectively , that are passed to them as environment variables ( here for local workers, here for cloud workers, and here for the frontend). Additionally, the frontend requires appropriate CORS middleware both on the user-facing API side and on the AWS S3 bucket side . Worker-facing API, user-facing API, user frontend all authenticate in an AWS Cognito user pool, defined in the AWS infrastructure repository . Users are divided into \"users\" and \"workers\"; additionally, cloud workers are identified by their membership in the \"cloud\" group. A user for the cloud workers is created with the CDK stack using a trigger . We typically store the Docker images of the supported applications in an AWS ECR repository (outside of the CDK stack). The AWS infrastructure repository contains a script to push local Docker images to a public ECR repository . Additionally, we provide scripts to link the APIs deployed on AWS to custom domains and setup the email sender for the user-facing API to send notifications to users . The AWS infrastructure repository deploys the APIs and Cloud Worker using their docker images published on ECR .","title":"Interdependencies"},{"location":"architecture/workflow/","text":"Job workflow In the example of the training of a DECODE network, the typical workflow of a job in DECODE OpenCloud looks like this: When submitting a job, users (can) select different input folders: Configuration : typically, a single file defining the job parameters (e.g., for DECODE v0.10, the folder in which the .yaml parameter file is found; for later versions of DECODE, this directory contains the whole hydra configuration). Data : folder(s) containing the input data (e.g., the bead calibration for DECODE training, and the input frames for DECODE fitting). Artifact : we call artifacts specific outputs of jobs (e.g., trained DECODE models) that will be used for later jobs (e.g., fitting data with DECODE).","title":"Job workflow"},{"location":"architecture/workflow/#job-workflow","text":"In the example of the training of a DECODE network, the typical workflow of a job in DECODE OpenCloud looks like this: When submitting a job, users (can) select different input folders: Configuration : typically, a single file defining the job parameters (e.g., for DECODE v0.10, the folder in which the .yaml parameter file is found; for later versions of DECODE, this directory contains the whole hydra configuration). Data : folder(s) containing the input data (e.g., the bead calibration for DECODE training, and the input frames for DECODE fitting). Artifact : we call artifacts specific outputs of jobs (e.g., trained DECODE models) that will be used for later jobs (e.g., fitting data with DECODE).","title":"Job workflow"},{"location":"developer/applications/","text":"Applications Currently supported applications The currently(*) supported applications are: DECODE : single-molecule localization microscopy DECODE Plex : multi-channel single-molecule localization microscopy Comet : drift correction (*) Please note that the ultimate source of truth for this is the /jobs/applications/ endpoint of the user-facing API . Publishing a new application See the instructions on the AWS infrastructure repository .","title":"Applications"},{"location":"developer/applications/#applications","text":"","title":"Applications"},{"location":"developer/applications/#currently-supported-applications","text":"The currently(*) supported applications are: DECODE : single-molecule localization microscopy DECODE Plex : multi-channel single-molecule localization microscopy Comet : drift correction (*) Please note that the ultimate source of truth for this is the /jobs/applications/ endpoint of the user-facing API .","title":"Currently supported applications"},{"location":"developer/applications/#publishing-a-new-application","text":"See the instructions on the AWS infrastructure repository .","title":"Publishing a new application"},{"location":"developer/debugging/","text":"Debugging This list is meant to be filled as problems arise and we find fixes. Debugging jobs Note that the IDs in the user-facing and worker-facing APIs are not the same. Jobs in the worker-facing API contain the job ID in the user-facing API in the field [\"meta\"][\"job_id\"] . Cloud job not starting Check the following: Did any pre- or post-processing Lambda function fail? On the AWS CLI, go to Lambda > Functions > <function> > Monitor and see if there were any failures. Is the job on AWS Batch stuck in the \"runnable\" state ( Batch > Job queue overview > <queue> )? This can happen if the allowed instance types do not have the resources requested by the job. See whether any instance is started by going to EC2 > Instances (running) . Frontend not changing after update You need to refresh the page, and possibly clear the browser history before reloading. After re-deploying, cloud jobs fail Check whether the problem is with mounting EFS. If a new filesystem is created, you need to de-register all old AWS Batch job definitions, since they still try to mount the old filesystem. Cloud job stuck on pre- or post-processing The lambdas might be failing. To debug: Test the lambda functions manually. Add the BasicExecutionRole to have them log to CloudWatch, then look at the logs. In particular, the lambdas might have too little memory.","title":"Debugging"},{"location":"developer/debugging/#debugging","text":"This list is meant to be filled as problems arise and we find fixes.","title":"Debugging"},{"location":"developer/debugging/#debugging-jobs","text":"Note that the IDs in the user-facing and worker-facing APIs are not the same. Jobs in the worker-facing API contain the job ID in the user-facing API in the field [\"meta\"][\"job_id\"] .","title":"Debugging jobs"},{"location":"developer/debugging/#cloud-job-not-starting","text":"Check the following: Did any pre- or post-processing Lambda function fail? On the AWS CLI, go to Lambda > Functions > <function> > Monitor and see if there were any failures. Is the job on AWS Batch stuck in the \"runnable\" state ( Batch > Job queue overview > <queue> )? This can happen if the allowed instance types do not have the resources requested by the job. See whether any instance is started by going to EC2 > Instances (running) .","title":"Cloud job not starting"},{"location":"developer/debugging/#frontend-not-changing-after-update","text":"You need to refresh the page, and possibly clear the browser history before reloading.","title":"Frontend not changing after update"},{"location":"developer/debugging/#after-re-deploying-cloud-jobs-fail","text":"Check whether the problem is with mounting EFS. If a new filesystem is created, you need to de-register all old AWS Batch job definitions, since they still try to mount the old filesystem.","title":"After re-deploying, cloud jobs fail"},{"location":"developer/debugging/#cloud-job-stuck-on-pre-or-post-processing","text":"The lambdas might be failing. To debug: Test the lambda functions manually. Add the BasicExecutionRole to have them log to CloudWatch, then look at the logs. In particular, the lambdas might have too little memory.","title":"Cloud job stuck on pre- or post-processing"},{"location":"developer/developing/","text":"Developing Environments We have a production (prod) and a development (dev) environment. Each environment has its own APIs and AWS stack. The application images, as they are not part of the stack itself, are common to both environments. However, the batch job definitions are separate. [not yet implemented] The prod stack follows the prod branch in each repository, the dev stack follows the main branch in each repository. The prod stack is always deployed, the dev stack can be deleted to save resources when not required. Dependencies handling We try to use similar structures in all repositories for consistency. In particular, we use poetry for dependency handling. Additionally, we use the scripts functionality of poetry to more easily build/run/delete/cleanup docker images and serve the APIs. Code checks All repositories use two required checks on PRs: Static code checks ruff and ruff format poetry (in strict mode) Tests using pytest Some tests require AWS credentials for a test user with the required permissions to test resources. For this, we use secrets in the GitHub repositories. The checks can also be ran manually: pre-commit install to automatically run the static code checks on commit pytest tests/ [-m \"aws or not(aws)\"] to automatically run the tests (the aws marker tags tests that require AWS credentials) Releases and deployment The versions of the applications (user-facing API, worker-facing API, job fetcher) are managed using poetry : To bump the version in pyproject.toml , run poetry version major|minor|patch . When a commit is merged and the version in pyproject.toml was changed: A docker image is published to AWS ECR (public); A github release is created. Alternatively, one can manually run the publish-version GH action for a docker build that can be manually deployed/tested. Deployment is done manually (both for dev and for prod) via the AWS_Infrastructure repository using the Python AWS CDK. Two yaml files ( config_prod.yaml and config_dev.yaml ) configure the two stacks. In the config, the version of the APIs and job fetcher are set (so that they can be different for prod and dev ). To test a new version of the API on dev, one does not necessarily need to run the cloud formation tools; instead, they can simply update the docker image setting of the corresponding AppRunner Service on the AWS console. Updating the JobFetcher version for workers is the task of the workers themselves.","title":"Developing"},{"location":"developer/developing/#developing","text":"","title":"Developing"},{"location":"developer/developing/#environments","text":"We have a production (prod) and a development (dev) environment. Each environment has its own APIs and AWS stack. The application images, as they are not part of the stack itself, are common to both environments. However, the batch job definitions are separate. [not yet implemented] The prod stack follows the prod branch in each repository, the dev stack follows the main branch in each repository. The prod stack is always deployed, the dev stack can be deleted to save resources when not required.","title":"Environments"},{"location":"developer/developing/#dependencies-handling","text":"We try to use similar structures in all repositories for consistency. In particular, we use poetry for dependency handling. Additionally, we use the scripts functionality of poetry to more easily build/run/delete/cleanup docker images and serve the APIs.","title":"Dependencies handling"},{"location":"developer/developing/#code-checks","text":"All repositories use two required checks on PRs: Static code checks ruff and ruff format poetry (in strict mode) Tests using pytest Some tests require AWS credentials for a test user with the required permissions to test resources. For this, we use secrets in the GitHub repositories. The checks can also be ran manually: pre-commit install to automatically run the static code checks on commit pytest tests/ [-m \"aws or not(aws)\"] to automatically run the tests (the aws marker tags tests that require AWS credentials)","title":"Code checks"},{"location":"developer/developing/#releases-and-deployment","text":"The versions of the applications (user-facing API, worker-facing API, job fetcher) are managed using poetry : To bump the version in pyproject.toml , run poetry version major|minor|patch . When a commit is merged and the version in pyproject.toml was changed: A docker image is published to AWS ECR (public); A github release is created. Alternatively, one can manually run the publish-version GH action for a docker build that can be manually deployed/tested. Deployment is done manually (both for dev and for prod) via the AWS_Infrastructure repository using the Python AWS CDK. Two yaml files ( config_prod.yaml and config_dev.yaml ) configure the two stacks. In the config, the version of the APIs and job fetcher are set (so that they can be different for prod and dev ). To test a new version of the API on dev, one does not necessarily need to run the cloud formation tools; instead, they can simply update the docker image setting of the corresponding AppRunner Service on the AWS console. Updating the JobFetcher version for workers is the task of the workers themselves.","title":"Releases and deployment"},{"location":"developer/testing/","text":"Testing Integration test See the integration tests repository : it can test both local and cloud workers. Manual integration testing The Run tests GitHub action allows testing a simple job workflow. You can specify which branches of the different repositories to use to integration-test coordinated changes in multiple repositories before merging them. The same process can be done locally using the pytest tests in the integration tests repository . This requires locally cloned repositories, checked out to the changes to test. On the background, docker images are tested and spin up with docker-compose . If you want to do manual testing of different configurations, you can simply spin up the docker containers (images automatically re-built) using docker-compose up [-d] . Automated integration test Currently, the Run tests GitHub action runs once a week testing a DECODE v0.10.1 job on the prod stack with a cloud worker with GPU. Unit tests See the respective repositories. They are mostly required on PRs. Repository integration tests (and sometimes even unit tests) require a testing AWS account and a user with some permissions on testing S3 buckets/RDS instances. We store credentials for them in GH's organization secrets, so that they are common across repositories. However, those tests mostly needed to be ran once manually with a role with more permissions, since we do not want to give GH actions the permission to e.g. create RDS instances (this is also mentioned in the repositories affected). Currently, the permissions given to the GH runner testing user are (might not be up-to-date): // DecodeCloudTestsEC2Ingress { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:RevokeSecurityGroupIngress\", \"ec2:AuthorizeSecurityGroupIngress\" ], \"Resource\": \"arn:aws:ec2:eu-central-1:<aws-account-id>:security-group/<sg-id>\" } ] } // DecodeCloudTestsRDS { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": \"rds:DescribeDBInstances\", \"Resource\": [ \"arn:aws:rds:*:<aws-account-id>:db:decodecloudintegrationtests\", \"arn:aws:rds:*:<aws-account-id>:db:decodecloudqueuetests\", \"arn:aws:rds:eu-central-1:<aws-account-id>:db:decodecloudintegrationtestsuserapi\" ] } ] } // DecodeCloudTestsS3Buckets { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::decode-cloud-integration-tests\", \"arn:aws:s3:::decode-cloud-integration-tests/*\", \"arn:aws:s3:::decode-cloud-filesystem-tests\", \"arn:aws:s3:::decode-cloud-filesystem-tests/*\", \"arn:aws:s3:::decode-cloud-user-integration-tests\", \"arn:aws:s3:::decode-cloud-user-integration-tests/*\", \"arn:aws:s3:::decode-cloud-user-filesystem-tests\", \"arn:aws:s3:::decode-cloud-user-filesystem-tests/*\" ] } ] } // DecodeCloudTestsSecrets { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:eu-central-1:<aws-account-id>:secret:decode-cloud-tests-db-pwd*\" } ] } // SQSTestingDecodeCloud { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": \"sqs:ListQueues\", \"Resource\": \"*\" }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": \"sqs:*\", \"Resource\": [ \"arn:aws:sqs:*:<aws-account-id>:cloud_queue.fifo\", \"arn:aws:sqs:*:<aws-account-id>:local_queue.fifo\", \"arn:aws:sqs:*:<aws-account-id>:None_queue.fifo\" ] } ] } Manual tests Testing user workflow without the frontend This is essentially what is done by the integration test. To do it manually using the Swagger docs of the user-facing API: Post a new user: { \"email\": \"user@example.com\", \"password\": \"Password1+\" } On the AWS console, verify this new user. Post a token and authenticate. Post an example parameter file and an example bead calibration (see for example the files in the DECODE tutorial ). In the example below, they are posted under test/param.yaml and test/calib.mat . Post a job: { \"job_name\": \"decode_test\", \"environment\": \"cloud\", \"priority\": 0, \"application\": { \"application\": \"decode\", \"version\": \"v0_10_1\", \"entrypoint\": \"train\" }, \"attributes\": { \"files_down\": { \"config_id\": \"test\", \"data_ids\": [\"test\"], \"artifact_ids\": [ ] }, \"env_vars\": { } }, \"hardware\": { \"cpu_cores\": null, \"memory\": null, \"gpu_model\": null, \"gpu_archi\": null, \"gpu_mem\": null } } You can try pulling the job from the deployed worker-facing API using the Swagger docs as well (worker authentication similar to the user authentication -- you can get the token using the worker-facing API). Testing lambda functions Use the AWS CLI.","title":"Testing"},{"location":"developer/testing/#testing","text":"","title":"Testing"},{"location":"developer/testing/#integration-test","text":"See the integration tests repository : it can test both local and cloud workers.","title":"Integration test"},{"location":"developer/testing/#manual-integration-testing","text":"The Run tests GitHub action allows testing a simple job workflow. You can specify which branches of the different repositories to use to integration-test coordinated changes in multiple repositories before merging them. The same process can be done locally using the pytest tests in the integration tests repository . This requires locally cloned repositories, checked out to the changes to test. On the background, docker images are tested and spin up with docker-compose . If you want to do manual testing of different configurations, you can simply spin up the docker containers (images automatically re-built) using docker-compose up [-d] .","title":"Manual integration testing"},{"location":"developer/testing/#automated-integration-test","text":"Currently, the Run tests GitHub action runs once a week testing a DECODE v0.10.1 job on the prod stack with a cloud worker with GPU.","title":"Automated integration test"},{"location":"developer/testing/#unit-tests","text":"See the respective repositories. They are mostly required on PRs. Repository integration tests (and sometimes even unit tests) require a testing AWS account and a user with some permissions on testing S3 buckets/RDS instances. We store credentials for them in GH's organization secrets, so that they are common across repositories. However, those tests mostly needed to be ran once manually with a role with more permissions, since we do not want to give GH actions the permission to e.g. create RDS instances (this is also mentioned in the repositories affected). Currently, the permissions given to the GH runner testing user are (might not be up-to-date): // DecodeCloudTestsEC2Ingress { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:RevokeSecurityGroupIngress\", \"ec2:AuthorizeSecurityGroupIngress\" ], \"Resource\": \"arn:aws:ec2:eu-central-1:<aws-account-id>:security-group/<sg-id>\" } ] } // DecodeCloudTestsRDS { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": \"rds:DescribeDBInstances\", \"Resource\": [ \"arn:aws:rds:*:<aws-account-id>:db:decodecloudintegrationtests\", \"arn:aws:rds:*:<aws-account-id>:db:decodecloudqueuetests\", \"arn:aws:rds:eu-central-1:<aws-account-id>:db:decodecloudintegrationtestsuserapi\" ] } ] } // DecodeCloudTestsS3Buckets { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::decode-cloud-integration-tests\", \"arn:aws:s3:::decode-cloud-integration-tests/*\", \"arn:aws:s3:::decode-cloud-filesystem-tests\", \"arn:aws:s3:::decode-cloud-filesystem-tests/*\", \"arn:aws:s3:::decode-cloud-user-integration-tests\", \"arn:aws:s3:::decode-cloud-user-integration-tests/*\", \"arn:aws:s3:::decode-cloud-user-filesystem-tests\", \"arn:aws:s3:::decode-cloud-user-filesystem-tests/*\" ] } ] } // DecodeCloudTestsSecrets { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:eu-central-1:<aws-account-id>:secret:decode-cloud-tests-db-pwd*\" } ] } // SQSTestingDecodeCloud { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": \"sqs:ListQueues\", \"Resource\": \"*\" }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": \"sqs:*\", \"Resource\": [ \"arn:aws:sqs:*:<aws-account-id>:cloud_queue.fifo\", \"arn:aws:sqs:*:<aws-account-id>:local_queue.fifo\", \"arn:aws:sqs:*:<aws-account-id>:None_queue.fifo\" ] } ] }","title":"Unit tests"},{"location":"developer/testing/#manual-tests","text":"","title":"Manual tests"},{"location":"developer/testing/#testing-user-workflow-without-the-frontend","text":"This is essentially what is done by the integration test. To do it manually using the Swagger docs of the user-facing API: Post a new user: { \"email\": \"user@example.com\", \"password\": \"Password1+\" } On the AWS console, verify this new user. Post a token and authenticate. Post an example parameter file and an example bead calibration (see for example the files in the DECODE tutorial ). In the example below, they are posted under test/param.yaml and test/calib.mat . Post a job: { \"job_name\": \"decode_test\", \"environment\": \"cloud\", \"priority\": 0, \"application\": { \"application\": \"decode\", \"version\": \"v0_10_1\", \"entrypoint\": \"train\" }, \"attributes\": { \"files_down\": { \"config_id\": \"test\", \"data_ids\": [\"test\"], \"artifact_ids\": [ ] }, \"env_vars\": { } }, \"hardware\": { \"cpu_cores\": null, \"memory\": null, \"gpu_model\": null, \"gpu_archi\": null, \"gpu_mem\": null } } You can try pulling the job from the deployed worker-facing API using the Swagger docs as well (worker authentication similar to the user authentication -- you can get the token using the worker-facing API).","title":"Testing user workflow without the frontend"},{"location":"developer/testing/#testing-lambda-functions","text":"Use the AWS CLI.","title":"Testing lambda functions"},{"location":"developer/users/","text":"Users Users handling Users and workers are both stored in an AWS Cognito user pool. This pool contains different groups: A \"users\" group, of which confirmed users of DECODE OpenCloud are members. Membership in this group is checked when authenticating in the user-facing API . A \"workers\" group, of which confirmed workers of DECODE OpenCloud are members. Membership in this group is checked when authenticating in the worker-facing API . A \"cloud\" group, of which cloud workers of DECODE OpenCloud are members. In particular, the user used by the AWS Batch workers is a member of this group. Registration workflow User view Potential users and workers can register using the \"Registration\" page of the user frontend . They provide their email address, password, and possibly add some details about the registration request (e.g., who they are, why they would like to have access, if they are registering a worker). They then need to verify the provided email address by using a verification code. After successful registration, users are in a state of pending confirmation: At login in the user frontend , they will be informed that their account still has to be confirmed by the administrators. Once the account is confirmed, they are able to use the frontend and APIs. Administrators view Confirmation of new users and workers is controlled by requiring their membership in the \"users\" and \"workers\" groups in the AWS Cognito user pool. As long as this is not the case, their access to the APIs is blocked. When a new user registers using the user frontend , an email is sent to the administrator email using a Cognito post-creation trigger . The administrator can see the request details on the AWS CLI and accept the request by adding the Cognito users to the appropriate groups.","title":"Users"},{"location":"developer/users/#users","text":"","title":"Users"},{"location":"developer/users/#users-handling","text":"Users and workers are both stored in an AWS Cognito user pool. This pool contains different groups: A \"users\" group, of which confirmed users of DECODE OpenCloud are members. Membership in this group is checked when authenticating in the user-facing API . A \"workers\" group, of which confirmed workers of DECODE OpenCloud are members. Membership in this group is checked when authenticating in the worker-facing API . A \"cloud\" group, of which cloud workers of DECODE OpenCloud are members. In particular, the user used by the AWS Batch workers is a member of this group.","title":"Users handling"},{"location":"developer/users/#registration-workflow","text":"","title":"Registration workflow"},{"location":"developer/users/#user-view","text":"Potential users and workers can register using the \"Registration\" page of the user frontend . They provide their email address, password, and possibly add some details about the registration request (e.g., who they are, why they would like to have access, if they are registering a worker). They then need to verify the provided email address by using a verification code. After successful registration, users are in a state of pending confirmation: At login in the user frontend , they will be informed that their account still has to be confirmed by the administrators. Once the account is confirmed, they are able to use the frontend and APIs.","title":"User view"},{"location":"developer/users/#administrators-view","text":"Confirmation of new users and workers is controlled by requiring their membership in the \"users\" and \"workers\" groups in the AWS Cognito user pool. As long as this is not the case, their access to the APIs is blocked. When a new user registers using the user frontend , an email is sent to the administrator email using a Cognito post-creation trigger . The administrator can see the request details on the AWS CLI and accept the request by adding the Cognito users to the appropriate groups.","title":"Administrators view"}]}